---
title: "End-to-End Workflow: Tuning and Training Random Machines"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{End-to-End Workflow: Tuning and Training Random Machines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  eval     = TRUE
)
```

# Introduction ðŸš€

This vignette presents a **fully executable**, end-to-end workflow for building  
**Random Survival Machines** with the `FastSurvivalSVM` package.

To guarantee that the vignette **runs during rendering** (CRAN / pkgdown / local),
we deliberately use:

- small sample size,
- small tuning grids,
- `cores = 1` everywhere.

The goal is **correctness and clarity**, not maximum performance.

# 1. Data Preparation ðŸ“¦

```{r}
library(FastSurvivalSVM)

set.seed(42)

df <- data_generation(n = 120, prop_cen = 0.20)

train_idx <- sample(seq_len(nrow(df)), 80)
train_df  <- df[train_idx, ]
test_df   <- df[-train_idx, ]

head(train_df)
```

# 2. Custom Kernels ðŸ§ 

## 2.1 Wavelet Kernel ðŸŒŠ

```{r}
wavelet_kernel <- function(x, z, A) {
  u <- (as.numeric(x) - as.numeric(z)) / A
  prod(cos(1.75 * u) * exp(-0.5 * u^2))
}
```

## 2.2 Polynomial Kernel ðŸ§®

```{r}
poly_kernel <- function(x, z, degree, coef0) {
  (sum(as.numeric(x) * as.numeric(z)) + coef0)^degree
}
```

# 3. Tuning a Single SVM with tune_fastsvm() ðŸ”§

```{r}
grid_rbf <- list(
  kernel     = "rbf",
  rank_ratio = 0,
  alpha      = c(0.1, 1),
  gamma      = c(0.01, 0.1)
)

rbf_tune <- tune_fastsvm(
  data       = train_df,
  time_col   = "tempo",
  delta_col  = "cens",
  param_grid = grid_rbf,
  cv         = 3,
  cores      = 1,
  verbose    = 0
)

rbf_tune
```

# 4. Kernel Mix and Parameter Grids ðŸ§©

```{r}
kernel_mix <- list(
  linear_std = list(kernel = "linear", rank_ratio = 0),
  rbf_std    = list(kernel = "rbf",    rank_ratio = 0),
  wavelet_k  = list(rank_ratio = 0),
  poly_k     = list(rank_ratio = 0)
)

param_grids <- list(
  linear_std = list(alpha = c(0.1, 1)),
  rbf_std    = list(alpha = c(0.1, 1), gamma = c(0.01)),
  wavelet_k  = list(
    kernel = grid_kernel(wavelet_kernel, A = c(1)),
    alpha  = c(0.1, 1)
  ),
  poly_k = list(
    kernel = grid_kernel(poly_kernel, degree = c(2), coef0 = 1),
    alpha  = c(0.1, 1)
  )
)
```

# 5. Joint Tuning with tune_random_machines() ðŸ”

```{r}
tune_res <- tune_random_machines(
  data        = train_df,
  time_col    = "tempo",
  delta_col   = "cens",
  kernel_mix  = kernel_mix,
  param_grids = param_grids,
  cv          = 3,
  cores       = 1,
  verbose     = 0
)

tune_res
```

# 6. Bridging with as_kernels() ðŸŒ‰

```{r}
final_kernels <- as_kernels(tune_res, kernel_mix)
final_kernels
```

# 7. Training Random Machines ðŸšœ

```{r}
rm_model <- random_machines(
  data         = train_df,
  newdata      = test_df,
  time_col     = "tempo",
  delta_col    = "cens",
  kernels      = final_kernels,
  B            = 5,
  mtry         = 2,
  crop         = 0.1,
  prop_holdout = 0.20,
  cores        = 1,
  .progress    = FALSE
)

rm_model
```

# 8. Final Evaluation ðŸŽ¯

```{r}
c_index <- score(rm_model, test_df)
cat(sprintf("Final C-index on test data: %.4f\n", c_index))
```

# Conclusion ðŸŽ‰

This vignette intentionally uses **small grids and datasets** so that:

- every chunk is executed,
- the vignette renders reliably,
- users can clearly see the *full workflow*.

For real experiments, simply increase:
`n`, tuning grids, `B`, and `cores`.
