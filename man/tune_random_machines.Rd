% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune.R
\name{tune_random_machines}
\alias{tune_random_machines}
\title{Multi-Kernel Tuning for Random Machines}
\usage{
tune_random_machines(
  data,
  time_col = "t",
  delta_col = "delta",
  kernel_mix,
  param_grids,
  cv = 5L,
  cores = parallel::detectCores(),
  verbose = 0L,
  ...
)
}
\arguments{
\item{data}{Training data frame.}

\item{time_col}{Time column name.}

\item{delta_col}{Event column name.}

\item{kernel_mix}{Named list of base kernel configurations (e.g. \code{list(rbf=list(kernel="rbf"))}).}

\item{param_grids}{Named list of parameter grids corresponding to \code{kernel_mix}.}

\item{cv}{Number of folds (default 5).}

\item{cores}{Number of parallel cores (default: \code{parallel::detectCores()}).}

\item{verbose}{Verbosity level (0 or 1).}

\item{...}{Additional fixed parameters passed to all estimators.}
}
\value{
An object of class \code{"random_machines_tune"}.
}
\description{
Orchestrates hyperparameter tuning for multiple kernels simultaneously.
This function allows mixing **native scikit-learn kernels** (string based)
and **custom R kernels** (function based) in a single tuning session.
}
\section{Parallelization Details}{

The \code{cores} parameter is passed down to the individual tuning function.
\itemize{
  \item For native kernels (strings), parallelism is handled via Scikit-Learn (efficient multithreading).
  \item For custom kernels (R functions), parallelism is managed via \pkg{mirai} (R multiprocessing).
}
}

\examples{
\dontrun{
if (reticulate::py_module_available("sksurv") && requireNamespace("mirai", quietly=TRUE)) {
  library(FastSurvivalSVM)
  
  set.seed(99)
  df <- data_generation(n = 200, prop_cen = 0.25)

  # =========================================================================
  # Setup: Hybrid Tuning (Native + Custom Kernels)
  # =========================================================================
  
  # 1. Prepare Custom Kernel Variants (e.g. Wavelet)
  make_wavelet <- function(A = 1) {
    force(A)
    function(x, z) {
      u <- (as.numeric(x) - as.numeric(z)) / A
      prod(cos(1.75 * u) * exp(-0.5 * u^2))
    }
  }
  wav_variants <- create_kernel_variants(make_wavelet, A = c(0.5, 2.0))

  # 2. Define Base Configurations (The "Mix")
  #    Notice we can mix "rbf" (string) and custom functions.
  mix <- list(
    # A. Native Scikit-learn Kernel
    my_rbf = list(
      kernel = "rbf",
      rank_ratio = 0.5  # Fixed param for this kernel
    ),
    
    # B. Native Linear Kernel
    my_linear = list(
      kernel = "linear",
      rank_ratio = 0.0
    ),
    
    # C. Custom R Kernel
    my_wavelet = list(
      # We don't set 'kernel' here because we will tune it in the grid
      rank_ratio = 1.0
    )
  )

  # 3. Define Grids for each member of the Mix
  #    The names must match the 'mix' list.
  grids <- list(
    # Tune alpha and gamma for RBF
    my_rbf = list(
      alpha = c(0.1, 10),
      gamma = c(0.01, 0.1)
    ),
    
    # Tune only alpha for Linear
    my_linear = list(
      alpha = c(0.01, 1)
    ),
    
    # Tune the kernel function itself (A=0.5 vs A=2.0) and alpha
    my_wavelet = list(
      kernel = wav_variants,
      alpha  = c(0.1, 1)
    )
  )

  # 4. Run Hybrid Tuning
  #    - 'my_rbf' and 'my_linear' will use Python parallelism (fast)
  #    - 'my_wavelet' will use mirai parallelism (robust)
  tune_results <- tune_random_machines(
    data = df,
    time_col = "tempo", delta_col = "cens",
    kernel_mix = mix,
    param_grids = grids,
    cv = 3,
    cores = 2,
    verbose = 1
  )
  
  print(tune_results)
}
}
}
