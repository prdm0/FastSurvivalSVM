---
title: "Robust Parallel Bagging with Mixed Kernels"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Robust Parallel Bagging}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

The **FastSurvivalSVM** package provides a robust implementation of **Bagging (Bootstrap Aggregating)** for Survival Support Vector Machines.

This implementation is designed for high-performance computing using:

1.  **`mirai`**: For robust, isolated parallel processing (preventing environment issues).
2.  **`reticulate`**: To interface with Python's `scikit-survival`.
3.  **Hybrid Architecture**: It supports mixing standard optimized kernels (C++) with custom user-defined R functions.

This vignette demonstrates a complete workflow: generating data, defining custom kernels, running a parallel ensemble for time regression, and evaluating performance.

> **Note:** This vignette executes parallel code. Ensure you have Python installed and configured via `reticulate`.

---

# 1. Data Generation and Splitting

First, we generate a synthetic dataset using a Weibull model with right-censoring. We split the data into a **training set** (used to build the bagging ensemble) and a **test set** (used to evaluate the final performance).

```{r setup, message=FALSE, warning=FALSE}
library(FastSurvivalSVM)
library(survival)

# Ensure reproducible data generation
set.seed(123)
df <- data_generation(n = 300, prop_cen = 0.3)

# Split: 200 for training, 100 for testing
idx <- sample(1:nrow(df), 200)
train_df <- df[idx, ]
test_df  <- df[-idx, ]

cat("Training set size:", nrow(train_df), "\n")
cat("Test set size:", nrow(test_df), "\n")
```

---

# 2. Defining Custom Kernels

One of the most powerful features of `fastsvm_bagging` is the ability to mix standard kernels (defined by strings like `"linear"` or `"rbf"`) with **custom kernels defined as R functions**.

Here we define two custom kernel factories. A "factory" is a function that takes hyperparameters (like `sigma` or `degree`) and returns a kernel function `f(x, z)`.

### Wavelet Kernel
This kernel is useful for capturing complex non-linear patterns.
\[ K(x, z) = \prod_{j=1}^p \cos\left(1.75 \frac{x_j - z_j}{A}\right) \exp\left(-0.5 \frac{(x_j - z_j)^2}{A^2}\right) \]

```{r}
# Factory for Wavelet Kernel
# Matches reference logic: cos(1.75u)*exp(-0.5u^2)
make_wavelet <- function(A = 1) {
  force(A) # Ensure A is captured
  function(x, z) {
    u <- (as.numeric(x) - as.numeric(z)) / A
    prod(cos(1.75 * u) * exp(-0.5 * u^2))
  }
}
```

### Custom Polynomial Kernel
A standard polynomial kernel implemented manually in R.
\[ K(x, z) = (\gamma \cdot x^T z + \text{coef0})^{\text{degree}} \]

```{r}
# Factory for Polynomial Kernel
make_poly <- function(degree = 3, coef0 = 1) {
  force(degree); force(coef0)
  function(x, z) (sum(as.numeric(x) * as.numeric(z)) + coef0)^degree
}
```

---

# 3. Configuring the Kernel Mixture

We create a list of kernel configurations. The bagging algorithm will randomly sample one of these configurations for each bootstrap iteration based on their initial performance.

**Crucial Note on `rank_ratio`:**

* We set **`rank_ratio = 0`**. This tells the SVM to perform **Regression**.
* The model attempts to predict the **Survival Time** (or a transformation of it).
* Predictions will be **positive values** where *Higher Value = Longer Survival*.

```{r}
# 3. Kernel Specifications (rank_ratio=0 for Regression/Time)
kernel_mix <- list(
  # Standard Linear Kernel (String)
  linear = list(
    kernel = "linear", 
    alpha = 1, 
    rank_ratio = 0, 
    fit_intercept = TRUE
  ),
  
  # Standard Polynomial Kernel (String)
  # Note: Integers (2L) help avoid warnings in Python strict typing
  poly_std = list(
    kernel = "poly", 
    degree = 2L, 
    alpha = 1, 
    rank_ratio = 0, 
    fit_intercept = TRUE
  ),
  
  # Custom Wavelet Kernel (R Function)
  wavelet = list(
    kernel = make_wavelet(A = 1), 
    alpha = 1, 
    rank_ratio = 0, 
    fit_intercept = TRUE
  ),
  
  # Custom Polynomial Kernel (R Function)
  poly_fun = list(
    kernel = make_poly(degree = 2L), 
    alpha = 1, 
    rank_ratio = 0, 
    fit_intercept = TRUE
  )
)
```

---

# 4. Running the Parallel Bagging

We execute `fastsvm_bagging`. Note that we pass `newdata = test_df` during training. 

This utilizes the **Train-and-Predict** architecture: predictions are computed inside the parallel workers immediately after training, ensuring numerical stability and avoiding serialization errors with Python objects.

```{r}
# Check if Python/Mirai are available before running (for safe vignette build)
can_run <- reticulate::py_module_available("sksurv") && requireNamespace("mirai", quietly = TRUE)

if (can_run) {
  # 4. Run Bagging (Using all available cores)
  bag_results <- fastsvm_bagging(
    data       = train_df,
    newdata    = test_df,
    time_col   = "tempo",
    delta_col  = "cens",
    kernels    = kernel_mix,
    B          = 50,
    cores      = parallel::detectCores(),
    seed       = 99,
    .progress  = FALSE # Set to FALSE for cleaner vignette output
  )
  
  print(bag_results)
}
```

---

# 5. Results Analysis

If the model ran successfully, we can inspect the predictions and the performance.

### Predictions (Time Scale)

Since we used `rank_ratio = 0`, the predictions represent time scores. Higher values indicate longer expected survival.

```{r}
if (can_run) {
  cat("Preview of Predictions:\n")
  print(head(bag_results$preds))
}
```

### Performance Evaluation (C-index)

We use `score_fastsvm_bag` to calculate the Concordance Index. This function automatically detects that we used Regression mode (`rank_ratio = 0`) and handles the correlation direction correctly (inverting the sign if necessary so that the C-index reflects concordance with survival time).

```{r}
if (can_run) {
  # Score using the helper function
  cidx <- score_fastsvm_bag(bag_results, test_df)
  cat(sprintf("Test C-index: %.4f\n", cidx))
}
```

A C-index above 0.7 or 0.8 indicates a very strong predictive performance for this synthetic dataset, demonstrating that the ensemble successfully learned the non-linear patterns using the provided kernels.

---

# Conclusion

The `fastsvm_bagging` function provides a powerful, numerically stable, and parallelized way to fit survival models. By allowing custom R kernels and handling the complexity of Python serialization internally (by predicting within workers), it allows users to focus on modeling complex survival patterns with ease.