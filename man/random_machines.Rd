% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/random_machines.R
\name{random_machines}
\alias{random_machines}
\title{Parallel Bagging for FastKernelSurvivalSVM (Random Machines)}
\usage{
random_machines(
  data,
  newdata,
  time_col = "t",
  delta_col = "delta",
  kernels,
  B = 100L,
  mtry = NULL,
  crop = NULL,
  beta_kernel = 1,
  beta_bag = 1,
  cores = 1L,
  seed = NULL,
  prop_holdout = 0.2,
  .progress = TRUE
)
}
\arguments{
\item{data}{A \code{data.frame} containing training data.}

\item{newdata}{A \code{data.frame} containing test data for prediction.}

\item{time_col}{Name of the column with survival times.}

\item{delta_col}{Name of the column with the event indicator (1 = event, 0 = censored).}

\item{kernels}{A named list of kernel specifications. Each element must be a list
of arguments to the estimator (e.g., \code{kernel}, \code{alpha},
\code{rank_ratio}, \code{fit_intercept}, \code{max_iter}, etc.).}

\item{B}{Integer. Number of bootstrap samples.}

\item{mtry}{Integer or Numeric. Number of variables to randomly sample at each split.
\itemize{
  \item \code{NULL} (default): Use all variables.
  \item Integer >= 1: Select exactly \code{mtry} variables.
  \item Numeric < 1: Select \code{mtry} fraction of variables (e.g., 0.5 = 50\%).
}}

\item{crop}{Numeric or NULL. Threshold for kernel selection probabilities.
If provided (e.g., \code{0.10}), any kernel with a calculated weight
less than or equal to this value in the holdout phase will be
discarded (weight set to 0), and the remaining weights will be
rescaled to sum to 1.}

\item{beta_kernel}{Numeric. Temperature for kernel selection probabilities
(based on internal holdout C-index).}

\item{beta_bag}{Numeric. Temperature for ensemble weighting
(based on OOB C-index of each bootstrap model).}

\item{cores}{Integer. Number of parallel workers (via \code{mirai}).}

\item{seed}{Optional integer passed to \code{mirai::daemons} and used for
internal sampling.}

\item{prop_holdout}{Numeric in (0, 1). Proportion of the original training data
used as internal validation when computing the kernel selection weights.
For example, \code{prop_holdout = 0.20} means that 20\% of the rows are
used as validation and the remaining 80\% as training in the weighting phase.
If the dataset is too small for this split (either side < 10 rows),
the function falls back to resubstitution.}

\item{.progress}{Logical. Show progress bar for the bootstrap loop?}
}
\value{
An object of class \code{"random_machines"} containing:
  \itemize{
    \item \code{preds}: Numeric vector of aggregated predictions for \code{newdata}.
    \item \code{weights}: Vector of weights assigned to each bootstrap model.
    \item \code{chosen_kernels}: Vector of kernel names selected in each bootstrap.
    \item \code{c_indices}: Vector of OOB C-indices for each bootstrap.
    \item \code{rank_ratio}: Rank ratio stored from the first successful model.
    \item \code{time_col}, \code{delta_col}: Names of the survival columns.
    \item \code{ensemble}: List with serialized models (Python pickle), features and params.
    \item \code{mtry}: The mtry value used.
    \item \code{kernel_lambdas}: The kernel selection probabilities (fixed from holdout).
    \item \code{kernel_names}: The names of the kernels used in bagging.
    \item \code{prop_holdout}: The proportion used for the internal holdout split.
    \item \code{crop}: The crop threshold used.
  }
}
\description{
Fits an ensemble of models using bootstrap aggregation (bagging) and
computes predictions for \code{newdata}. This function is also referred to
as "Random Machines" in the context of kernel survival machines.
}
\details{
\strong{Internal Holdout for Kernel Weights:}
Instead of using training performance (resubstitution) to define the
selection probabilities of each kernel, this function splits the training
data into an internal training/validation set according to \code{prop_holdout}.
This mimics the behavior of the serial implementation where weights are
fixed based on a pre-bagging holdout.

\strong{Architecture:}
This function adopts a "Train-and-Predict" strategy:
\itemize{
  \item It accepts \code{newdata} and computes predictions inside the
        parallel workers (for each bootstrap model).
  \item It stores a serialized version (Python pickle) of each
        fitted model to allow future predictions via
        \code{\link{predict.random_machines}}.
}

\strong{Random Subspace (mtry):}
The \code{mtry} parameter allows for random selection of a subset of
covariates in each base learner, similar to Random Forests.
}
\examples{
\dontrun{
if (reticulate::py_module_available("sksurv") && requireNamespace("mirai")) {
  library(FastSurvivalSVM)

  ## 1. Data generation
  set.seed(3)
  df <- data_generation(n = 300, prop_cen = 0.3)

  ## Train/Test split
  idx <- sample(seq_len(nrow(df)), 200)
  train_df <- df[idx, ]
  test_df  <- df[-idx, ]

  ## 2. Custom Kernel Factories ----------------------------------
  make_wavelet <- function(A = 1) {
    force(A)
    function(x, z) {
      u <- (as.numeric(x) - as.numeric(z)) / A
      prod(cos(1.75 * u) * exp(-0.5 * u^2))
    }
  }

  make_poly <- function(degree = 3, coef0 = 1) {
    force(degree); force(coef0)
    function(x, z) (sum(as.numeric(x) * as.numeric(z)) + coef0)^degree
  }

  ## 3. Kernel Specifications (rank_ratio = 0 for Regression / Time)
  kernel_mix <- list(
    linear   = list(
      kernel        = "linear",
      alpha         = 1,
      rank_ratio    = 0,
      fit_intercept = TRUE
    ),
    rbf      = list(
      kernel        = "rbf",
      alpha         = 0.5,
      gamma         = 0.1,
      rank_ratio    = 0,
      fit_intercept = TRUE
    ),
    poly_std = list(
      kernel        = "poly",
      degree        = 2L,
      alpha         = 1,
      rank_ratio    = 0,
      fit_intercept = TRUE
    ),
    wavelet  = list(
      kernel        = make_wavelet(A = 1),
      alpha         = 1,
      rank_ratio    = 0,
      fit_intercept = TRUE
    ),
    poly_fun = list(
      kernel        = make_poly(degree = 2L),
      alpha         = 1,
      rank_ratio    = 0,
      fit_intercept = TRUE
    )
  )

  ## 4. Run Random Machines with internal holdout for kernel weights
  rm_results <- random_machines(
    data         = train_df,
    newdata      = test_df,
    time_col     = "tempo",
    delta_col    = "cens",
    kernels      = kernel_mix,
    B            = 50,
    mtry         = NULL,
    beta_kernel  = 1,
    beta_bag     = 1,
    cores        = parallel::detectCores(),
    seed         = 42,
    prop_holdout = 0.20,
    crop         = 0.15,  # Eliminate kernels with weight <= 0.15
    .progress    = TRUE
  )

  print(rm_results)

  ## 5. Score on independent test set using S3 method
  cidx_test <- score(rm_results, test_df)
  cat(sprintf("Test C-index (Random Machines): \%.4f\n", cidx_test))
}
}

}
