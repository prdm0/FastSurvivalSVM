% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bagging.R, R/bagging_backup.R
\name{fastsvm_bagging_fit}
\alias{fastsvm_bagging_fit}
\title{Bagging of FastKernelSurvivalSVM with covariate sub-sampling}
\usage{
fastsvm_bagging_fit(
  data,
  time_col = "t",
  delta_col = "delta",
  n_estimators = 50L,
  mtry = NULL,
  sample_frac = 1,
  replace = TRUE,
  kernels = "rbf",
  kernel_prob = NULL,
  alpha = 1,
  rank_ratio = 0,
  fit_intercept = FALSE,
  parallel = FALSE,
  seed = NULL,
  ...
)

fastsvm_bagging_fit(
  data,
  time_col = "t",
  delta_col = "delta",
  n_estimators = 50L,
  mtry = NULL,
  sample_frac = 1,
  replace = TRUE,
  kernels = "rbf",
  kernel_prob = NULL,
  alpha = 1,
  rank_ratio = 0,
  fit_intercept = FALSE,
  parallel = FALSE,
  seed = NULL,
  ...
)
}
\arguments{
\item{data}{A \code{data.frame} containing survival time, event indicator,
and covariates.}

\item{time_col}{Name of the column in \code{data} containing survival times.}

\item{delta_col}{Name of the column in \code{data} containing the event
indicator (1 = event, 0 = right censoring).}

\item{n_estimators}{Integer; number of base learners (bootstrap replicates)
in the ensemble.}

\item{mtry}{Integer; number of covariates to be used in each base learner.
If \code{NULL}, defaults to \code{floor(sqrt(p))}, where \code{p} is the
number of available covariates.}

\item{sample_frac}{Fraction of rows to sample for each base learner.
If \code{replace = TRUE}, \code{sample_frac = 1} corresponds to standard
bootstrap resampling. If smaller than 1, it performs subsampling.}

\item{replace}{Logical; if \code{TRUE}, sample rows with replacement
(bootstrap). If \code{FALSE}, sample without replacement.}

\item{kernels}{Kernel specification:
\itemize{
  \item A single value: either a character string (e.g. \code{"rbf"},
        \code{"poly"}) or an R function \code{function(x, z) ...}, used in
        all base models.
  \item A \code{list()} of kernels (each element a string or function);
        for each base model, one kernel is sampled from this list.
}
Kernel functions can be \emph{factory functions} that already fix their
hyperparameters (e.g. \code{make_wavelet_kernel(A = 0.5)}).}

\item{kernel_prob}{Optional numeric vector of sampling probabilities for
kernels when \code{kernels} is a list. Must have the same length as
\code{kernels}. If \code{NULL}, a uniform distribution over kernels is
used.}

\item{alpha}{Regularisation parameter for the SVM, passed to
\code{FastKernelSurvivalSVM(alpha = ...)}.}

\item{rank_ratio}{Mixing parameter between ranking and regression objectives
(\code{0 <= rank_ratio <= 1}), forwarded to the Python estimator.
For pure regression on transformed survival times, use \code{rank_ratio = 0}.}

\item{fit_intercept}{Logical; if \code{TRUE}, include an intercept when there
is a regression component (\code{rank_ratio < 1}).}

\item{parallel}{Logical; if \code{TRUE}, fit base learners in parallel using
\pkg{future.apply}. The execution plan must be set by the user using
\code{future::plan()}.}

\item{seed}{Optional integer; seed to be set before bagging. Useful for
reproducibility, especially together with \pkg{future}.}

\item{...}{Additional arguments passed directly to
\code{sksurv.svm.FastKernelSurvivalSVM()} via
\code{fast_kernel_surv_svm_fit()}, e.g. \code{gamma}, \code{degree},
\code{coef0}, \code{max_iter}, \code{optimizer}, \code{tol}, etc.}
}
\value{
An object of class \code{"fastsvm_bagging"} with components:
  \itemize{
    \item \code{models}: list of \code{"fastsvm"} base learners;
    \item \code{subspaces}: list of character vectors with the covariate
          names used in each base learner;
    \item \code{time_col}, \code{delta_col}, \code{x_cols}: metadata;
    \item \code{rank_ratio}: ensemble-level \code{rank_ratio};
    \item \code{n_estimators}, \code{mtry}, \code{sample_frac},
          \code{replace}: ensemble configuration.
  }

An object of class \code{"fastsvm_bagging"} with components:
  \itemize{
    \item \code{models}: list of \code{"fastsvm"} base learners;
    \item \code{subspaces}: list of character vectors with the covariate
          names used in each base learner;
    \item \code{time_col}, \code{delta_col}, \code{x_cols}: metadata;
    \item \code{rank_ratio}: ensemble-level \code{rank_ratio};
    \item \code{n_estimators}, \code{mtry}, \code{sample_frac},
          \code{replace}: ensemble configuration.
  }
}
\description{
Fit a bagging ensemble of \code{sksurv.svm.FastKernelSurvivalSVM} models,
using \code{fast_kernel_surv_svm_fit()} as the base learner.

Fit a bagging ensemble of \code{sksurv.svm.FastKernelSurvivalSVM} models,
using \code{fast_kernel_surv_svm_fit()} as the base learner.
}
\details{
Each base model is fitted on:
\itemize{
  \item a bootstrap (or subsample) of the rows, and
  \item a random subset of the covariates (\code{mtry}), analogous to
        Random Forest, but with kernel survival SVMs instead of trees.
}

The \code{kernels} argument controls how kernels are chosen:
\itemize{
  \item If a single value is provided (string or function), the same kernel
        is used in all base models.
  \item If a \code{list()} is provided, each element must be a valid kernel
        (a string or an R function compatible with
        \code{fast_kernel_surv_svm_fit()}), and for each base model one
        kernel is sampled from this list.
}

Machine-learning friendly design:
\itemize{
  \item SVM hyperparameters (\code{alpha}, \code{rank_ratio}, etc.) are
        explicit, named arguments of this function, suitable for external
        tuning (e.g., with \pkg{tidymodels}).
  \item Hyperparameters of custom kernels should be encoded inside the
        kernel functions themselves (e.g. \code{make_wavelet_kernel(A = 0.5)}).
        The bagging function does \emph{not} re-tune kernel hyperparameters
        at each bootstrap; it only samples among already parameterised
        kernels.
}

This package does not implement new kernel functions itself. The user is
expected to:
\itemize{
  \item use strings supported by scikit-learn
        (e.g. \code{"rbf"}, \code{"poly"}, \code{"sigmoid"},
        \code{"laplacian"}, etc.), or
  \item pass R functions of the form \code{function(x, z) ...} that return
        a scalar kernel value, as required by \code{FastKernelSurvivalSVM}.
}

Parallelisation:
\itemize{
  \item If \code{parallel = TRUE}, this function automatically configures
        a parallel backend using \pkg{future} and \pkg{future.apply}, and
        fits base learners in parallel via \code{future_lapply()}.
  \item On Unix/macOS, \code{future::multicore} is used; on Windows,
        \code{future::multisession} is used.
  \item All available cores (as reported by \code{parallel::detectCores()})
        are used as workers.
  \item The previous \code{future::plan()} is restored at the end of the
        function call, so the global plan is not permanently modified.
}

Each base model is fitted on:
\itemize{
  \item a bootstrap (or subsample) of the rows, and
  \item a random subset of the covariates (\code{mtry}), analogous to
        Random Forest, but with kernel survival SVMs instead of trees.
}

The \code{kernels} argument controls how kernels are chosen:
\itemize{
  \item If a single value is provided (string or function), the same kernel
        is used in all base models.
  \item If a \code{list()} is provided, each element must be a valid kernel
        (a string or an R function compatible with
        \code{fast_kernel_surv_svm_fit()}), and for each base model one
        kernel is sampled from this list.
}

Machine-learning friendly design:
\itemize{
  \item SVM hyperparameters (\code{alpha}, \code{rank_ratio}, etc.) are
        explicit, named arguments of this function, suitable for external
        tuning (e.g., with \pkg{tidymodels}).
  \item Hyperparameters of custom kernels should be encoded inside the
        kernel functions themselves (e.g. \code{make_wavelet_kernel(A = 0.5)}).
        The bagging function does \emph{not} re-tune kernel hyperparameters
        at each bootstrap; it only samples among already parameterised
        kernels.
}

This package does not implement new kernel functions itself. The user is
expected to:
\itemize{
  \item use strings supported by scikit-learn
        (e.g. \code{"rbf"}, \code{"poly"}, \code{"sigmoid"},
        \code{"laplacian"}, etc.), or
  \item pass R functions of the form \code{function(x, z) ...} that return
        a scalar kernel value, as required by \code{FastKernelSurvivalSVM}.
}

Parallelisation:
\itemize{
  \item If \code{parallel = TRUE}, base models are fitted in parallel using
        \pkg{future.apply} (\code{future_lapply}), allowing any backend
        supported by \pkg{future} (multicore, multisession, cluster, etc.).
  \item The user must set the execution plan before calling this function,
        e.g. \code{future::plan(multisession)}.
}
}
\examples{
if (reticulate::py_module_available("sksurv")) {

  ## ------------------------------------------------------------
  ## Example 1: Simple bagging with a fixed RBF kernel (regression mode)
  ## ------------------------------------------------------------
  set.seed(123)
  df <- data_generation(n = 300L, prop_cen = 0.10)

  bag_rbf <- fastsvm_bagging_fit(
    data         = df,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 10L,
    mtry         = 2L,
    kernels      = "rbf",   # scikit-learn RBF kernel
    alpha        = 1,
    rank_ratio   = 0        # pure regression on transformed times
  )

  preds_rbf <- predict(bag_rbf, df)
  head(preds_rbf)

  cindex_rbf <- score_fastsvm_bagging(bag_rbf, df)
  cindex_rbf


  ## ------------------------------------------------------------
  ## Example 2: Bagging with a list of kernels (RBF and polynomial)
  ## ------------------------------------------------------------
  kernels_list <- list("rbf", "poly")

  bag_mix <- fastsvm_bagging_fit(
    data         = df,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 15L,
    mtry         = 3L,
    kernels      = kernels_list,
    kernel_prob  = c(0.7, 0.3), # 70\% RBF, 30\% polynomial
    alpha        = 0.5,
    rank_ratio   = 0
  )

  preds_mix <- predict(bag_mix, df)
  head(preds_mix)

  cindex_mix <- score_fastsvm_bagging(bag_mix, df)
  cindex_mix


  ## ------------------------------------------------------------
  ## Example 3: Bagging with custom R kernel functions
  ##             (including hyperparameters)
  ## ------------------------------------------------------------

  # Custom wavelet mother function
  wavelet_mother <- function(u) {
    cos(1.75 * u) * exp(-0.5 * u^2)
  }

  # Multidimensional wavelet kernel with scale parameter A
  wavelet_kernel <- function(x, z, A = 1) {
    x <- as.numeric(x)
    z <- as.numeric(z)
    stopifnot(length(x) == length(z))
    stopifnot(length(A) == 1L, A > 0)

    u <- (x - z) / A
    prod(wavelet_mother(u))
  }

  # Kernel factory fixing the hyperparameter A
  make_wavelet_kernel <- function(A = 1) {
    force(A)
    function(x, z) wavelet_kernel(x, z, A = A)
  }

  # Another example: RBF kernel with tunable sigma
  make_rbf_kernel <- function(sigma = 1) {
    force(sigma)
    function(x, z) {
      x <- as.numeric(x); z <- as.numeric(z)
      d2 <- sum((x - z)^2)
      exp(-d2 / (2 * sigma^2))
    }
  }

  custom_kernels <- list(
    make_wavelet_kernel(A = 0.5),  # wavelet kernel with A = 0.5
    make_rbf_kernel(sigma = 0.8)   # custom RBF kernel with sigma = 0.8
  )

  bag_custom <- fastsvm_bagging_fit(
    data         = df,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 10L,
    mtry         = 2L,
    kernels      = custom_kernels,  # list of R kernel functions
    alpha        = 1,
    rank_ratio   = 0
  )

  preds_custom <- predict(bag_custom, df)
  head(preds_custom)

  cindex_custom <- score_fastsvm_bagging(bag_custom, df)
  cindex_custom


  ## ------------------------------------------------------------
  ## Example 4 (optional): Parallel fitting (internal plan handling)
  ## ------------------------------------------------------------
  # Here parallel = TRUE triggers automatic multicore/multisession setup
  set.seed(999)
  bag_parallel <- fastsvm_bagging_fit(
    data         = df,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 20L,
    mtry         = 2L,
    kernels      = "rbf",
    alpha        = 1,
    rank_ratio   = 0,
    parallel     = TRUE
  )
  score_fastsvm_bagging(bag_parallel, df)


  ## ------------------------------------------------------------
  ## Example 5: Comparing single FastKernelSurvivalSVM (poly)
  ##            vs bagging with randomisation of 3 kernels
  ## ------------------------------------------------------------

  set.seed(456)
  df_poly <- data_generation(n = 300L, prop_cen = 0.10)

  ## (a) Single-model FastKernelSurvivalSVM with polynomial kernel
  fit_poly <- fast_kernel_surv_svm_fit(
    data        = df_poly,
    time_col    = "tempo",
    delta_col   = "cens",
    kernel      = "poly",
    alpha       = 2,
    rank_ratio  = 0,
    degree      = 3L,   # integer required by scikit-learn
    coef0       = 1L
  )

  cindex_poly_single <- score_fastsvm(fit_poly, df_poly)


  ## (b) Bagging with 3 kernels: poly, rbf and wavelet

  # Wavelet mother function
  wavelet_mother2 <- function(u) {
    cos(1.75 * u) * exp(-0.5 * u^2)
  }

  # Wavelet kernel with hyperparameter A
  wavelet_kernel2 <- function(x, z, A = 1) {
    x <- as.numeric(x); z <- as.numeric(z)
    u <- (x - z) / A
    prod(wavelet_mother2(u))
  }

  # Kernel factory: fixes A and returns a function(x, z)
  make_wavelet_kernel2 <- function(A = 1) {
    force(A)
    function(x, z) wavelet_kernel2(x, z, A = A)
  }

  kernels_3 <- list(
    "poly",
    "rbf",
    make_wavelet_kernel2(A = 0.7)
  )

  bag_poly_mix <- fastsvm_bagging_fit(
    data         = df_poly,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 20L,
    mtry         = 2L,
    kernels      = kernels_3,
    kernel_prob  = c(0.4, 0.4, 0.2),  # 40\% poly, 40\% rbf, 20\% wavelet
    alpha        = 2,
    rank_ratio   = 0,
    degree       = 3L,                # used only for poly kernel
    coef0        = 1L
  )

  cindex_poly_bagging <- score_fastsvm_bagging(bag_poly_mix, df_poly)

  ## Compare C-index: single model vs bagging ensemble
  c(
    cindex_poly_single  = cindex_poly_single,
    cindex_poly_bagging = cindex_poly_bagging
  )

}

if (reticulate::py_module_available("sksurv")) {

  ## ------------------------------------------------------------
  ## Example 1: Simple bagging with a fixed RBF kernel (regression mode)
  ## ------------------------------------------------------------
  set.seed(123)
  df <- data_generation(n = 300L, prop_cen = 0.10)

  bag_rbf <- fastsvm_bagging_fit(
    data         = df,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 10L,
    mtry         = 2L,
    kernels      = "rbf",   # scikit-learn RBF kernel
    alpha        = 1,
    rank_ratio   = 0        # pure regression on transformed times
  )

  preds_rbf <- predict(bag_rbf, df)
  head(preds_rbf)

  cindex_rbf <- score_fastsvm_bagging(bag_rbf, df)
  cindex_rbf


  ## ------------------------------------------------------------
  ## Example 2: Bagging with a list of kernels (RBF and polynomial)
  ## ------------------------------------------------------------
  kernels_list <- list("rbf", "poly")

  bag_mix <- fastsvm_bagging_fit(
    data         = df,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 15L,
    mtry         = 3L,
    kernels      = kernels_list,
    kernel_prob  = c(0.7, 0.3), # 70\% RBF, 30\% polynomial
    alpha        = 0.5,
    rank_ratio   = 0
  )

  preds_mix <- predict(bag_mix, df)
  head(preds_mix)

  cindex_mix <- score_fastsvm_bagging(bag_mix, df)
  cindex_mix


  ## ------------------------------------------------------------
  ## Example 3: Bagging with custom R kernel functions
  ##             (including hyperparameters)
  ## ------------------------------------------------------------

  # Custom wavelet mother function
  wavelet_mother <- function(u) {
    cos(1.75 * u) * exp(-0.5 * u^2)
  }

  # Multidimensional wavelet kernel with scale parameter A
  wavelet_kernel <- function(x, z, A = 1) {
    x <- as.numeric(x)
    z <- as.numeric(z)
    stopifnot(length(x) == length(z))
    stopifnot(length(A) == 1L, A > 0)

    u <- (x - z) / A
    prod(wavelet_mother(u))
  }

  # Kernel factory fixing the hyperparameter A
  make_wavelet_kernel <- function(A = 1) {
    force(A)
    function(x, z) wavelet_kernel(x, z, A = A)
  }

  # Another example: RBF kernel with tunable sigma
  make_rbf_kernel <- function(sigma = 1) {
    force(sigma)
    function(x, z) {
      x <- as.numeric(x); z <- as.numeric(z)
      d2 <- sum((x - z)^2)
      exp(-d2 / (2 * sigma^2))
    }
  }

  custom_kernels <- list(
    make_wavelet_kernel(A = 0.5),  # wavelet kernel with A = 0.5
    make_rbf_kernel(sigma = 0.8)   # custom RBF kernel with sigma = 0.8
  )

  bag_custom <- fastsvm_bagging_fit(
    data         = df,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 10L,
    mtry         = 2L,
    kernels      = custom_kernels,  # list of R kernel functions
    alpha        = 1,
    rank_ratio   = 0
  )

  preds_custom <- predict(bag_custom, df)
  head(preds_custom)

  cindex_custom <- score_fastsvm_bagging(bag_custom, df)
  cindex_custom


  ## ------------------------------------------------------------
  ## Example 4 (optional): Parallel fitting using future/future.apply
  ## ------------------------------------------------------------
  ## Not run by default on CRAN / examples:
  # if (requireNamespace("future.apply", quietly = TRUE)) {
  #   future::plan(future::multisession)
  #   bag_parallel <- fastsvm_bagging_fit(
  #     data         = df,
  #     time_col     = "tempo",
  #     delta_col    = "cens",
  #     n_estimators = 20L,
  #     mtry         = 2L,
  #     kernels      = "rbf",
  #     alpha        = 1,
  #     rank_ratio   = 0,
  #     parallel     = TRUE,
  #     seed         = 999L
  #   )
  #   score_fastsvm_bagging(bag_parallel, df)
  # }


  ## ------------------------------------------------------------
  ## Example 5: Comparing single FastKernelSurvivalSVM (poly)
  ##            vs bagging with randomisation of 3 kernels
  ## ------------------------------------------------------------

  set.seed(456)
  df_poly <- data_generation(n = 300L, prop_cen = 0.10)

  ## (a) Single-model FastKernelSurvivalSVM with polynomial kernel
  fit_poly <- fast_kernel_surv_svm_fit(
    data        = df_poly,
    time_col    = "tempo",
    delta_col   = "cens",
    kernel      = "poly",
    alpha       = 2,
    rank_ratio  = 0,
    degree      = 3L,   # integer required by scikit-learn
    coef0       = 1L
  )

  cindex_poly_single <- score_fastsvm(fit_poly, df_poly)


  ## (b) Bagging with 3 kernels: poly, rbf and wavelet

  # Wavelet mother function
  wavelet_mother2 <- function(u) {
    cos(1.75 * u) * exp(-0.5 * u^2)
  }

  # Wavelet kernel with hyperparameter A
  wavelet_kernel2 <- function(x, z, A = 1) {
    x <- as.numeric(x); z <- as.numeric(z)
    u <- (x - z) / A
    prod(wavelet_mother2(u))
  }

  # Kernel factory: fixes A and returns a function(x, z)
  make_wavelet_kernel2 <- function(A = 1) {
    force(A)
    function(x, z) wavelet_kernel2(x, z, A = A)
  }

  kernels_3 <- list(
    "poly",
    "rbf",
    make_wavelet_kernel2(A = 0.7)
  )

  bag_poly_mix <- fastsvm_bagging_fit(
    data         = df_poly,
    time_col     = "tempo",
    delta_col    = "cens",
    n_estimators = 20L,
    mtry         = 2L,
    kernels      = kernels_3,
    kernel_prob  = c(0.4, 0.4, 0.2),  # 40\% poly, 40\% rbf, 20\% wavelet
    alpha        = 2,
    rank_ratio   = 0,
    degree       = 3L,                # used only for poly kernel
    coef0        = 1L
  )

  cindex_poly_bagging <- score_fastsvm_bagging(bag_poly_mix, df_poly)

  ## Compare C-index: single model vs bagging ensemble
  c(
    cindex_poly_single  = cindex_poly_single,
    cindex_poly_bagging = cindex_poly_bagging
  )

}
}
